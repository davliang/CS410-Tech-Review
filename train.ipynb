{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import HAN\n",
    "from src.dataset import HANDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yelp(file_path=\"data/yelp-2015.json\"):\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    df = df[[\"stars\", \"text\"]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_frac=0.8, eval_frac=0.1, test_frac=0.1, random_state=0):\n",
    "    # Ensure the fractions sum to 1.0\n",
    "    assert abs(train_frac + eval_frac + test_frac - 1.0) < 1e-6, (\n",
    "        \"Fractions must sum to 1.0\"\n",
    "    )\n",
    "\n",
    "    df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, test_size=(1 - train_frac), random_state=random_state\n",
    "    )\n",
    "    eval_df, test_df = train_test_split(\n",
    "        temp_df, test_size=0.5, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return train_df, eval_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, eval_dataloader, num_epochs=5, lr=1e-3, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Trains the model on the training set and evaluates on the validation set.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_docs, batch_labels in tqdm(train_dataloader):\n",
    "            batch_docs = batch_docs.to(device)\n",
    "            # Adjust labels from 1-5 to 0-4\n",
    "            batch_labels = (batch_labels - 1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits, _, _ = model(batch_docs)\n",
    "            loss = criterion(logits, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * batch_docs.size(0)\n",
    "        avg_loss = running_loss / len(train_dataloader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for val_docs, val_labels in eval_dataloader:\n",
    "                val_docs = val_docs.to(device)\n",
    "                val_labels = (val_labels - 1).to(device)\n",
    "                logits, _, _ = model(val_docs)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += (predictions == val_labels).sum().item()\n",
    "                total += val_labels.size(0)\n",
    "        val_acc = correct / total if total > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader, device=torch.device(\"cpu\")):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for test_docs, test_labels in test_dataloader:\n",
    "            test_docs = test_docs.to(device)\n",
    "            test_labels = (test_labels - 1).to(device)\n",
    "            logits, _, _ = model(test_docs)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == test_labels).sum().item()\n",
    "            total += test_labels.size(0)\n",
    "    test_acc = correct / total\n",
    "    print(\"Test Accuracy: {:.4f}\".format(test_acc))\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model, dataset, index, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Visualizes the attention weights for a single document from the dataset.\n",
    "    Assumes that the dataset has a 'vocab' attribute (word-to-index dictionary).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Get a sample document and its label.\n",
    "    doc_tensor, label = dataset[index]\n",
    "    # Add batch dimension: shape (1, max_sentences, max_sentence_length)\n",
    "    doc_tensor = doc_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass to get attention weights.\n",
    "    with torch.no_grad():\n",
    "        logits, word_attn_weights, sent_attn_weights = model(doc_tensor)\n",
    "        pred = torch.argmax(logits, dim=1).item() + 1  # adjust back to 1-5 scale\n",
    "\n",
    "    # Build an inverse vocabulary mapping (id -> token)\n",
    "    inv_vocab = {v: k for k, v in dataset.vocab.items()}\n",
    "    \n",
    "    # Reconstruct tokens from the document tensor.\n",
    "    doc_array = doc_tensor.squeeze(0).cpu().numpy()  # shape: (max_sentences, max_sentence_length)\n",
    "    doc_tokens = []\n",
    "    for sent in doc_array:\n",
    "        tokens = [inv_vocab.get(token_id, \"<UNK>\") for token_id in sent]\n",
    "        doc_tokens.append(tokens)\n",
    "    \n",
    "    # Plot Sentence-Level Attention.\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sent_attn = sent_attn_weights.squeeze(0).cpu().numpy()\n",
    "    sns.barplot(x=list(range(len(sent_attn))), y=sent_attn)\n",
    "    plt.title(\"Sentence Attention Weights\")\n",
    "    plt.xlabel(\"Sentence Index\")\n",
    "    plt.ylabel(\"Attention Weight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Word-Level Attention for each sentence.\n",
    "    # word_attn_weights shape: (batch_size*num_sentences, max_sentence_length)\n",
    "    word_attn = word_attn_weights.cpu().numpy().reshape(-1, doc_tensor.size(-1))\n",
    "    num_sentences = len(doc_tokens)\n",
    "    fig, axes = plt.subplots(num_sentences, 1, figsize=(12, num_sentences * 1.5))\n",
    "    if num_sentences == 1:\n",
    "        axes = [axes]\n",
    "    for i in range(num_sentences):\n",
    "        ax = axes[i]\n",
    "        tokens = doc_tokens[i]\n",
    "        attn = word_attn[i]\n",
    "        ax.bar(range(len(tokens)), attn, tick_label=tokens)\n",
    "        ax.set_title(f\"Word Attention for Sentence {i+1}\")\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"True Label: {label}, Predicted Label: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Yelp dataset...\n",
      "Loaded 1569264 samples from Yelp dataset.\n",
      "   stars                                               text\n",
      "0      5  dr. goldberg offers everything i look for in a...\n",
      "1      2  Unfortunately, the frustration of being Dr. Go...\n",
      "2      4  Dr. Goldberg has been my doctor for years and ...\n",
      "3      4  Been going to Dr. Goldberg for over 10 years. ...\n",
      "4      4  Got a letter in the mail last week that said D...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Yelp dataset...\")\n",
    "df = load_yelp()\n",
    "print(f\"Loaded {len(df)} samples from Yelp dataset.\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train, eval, and test sets...\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting data into train, eval, and test sets...\")\n",
    "train_df, eval_df, test_df = split_data(df)\n",
    "\n",
    "train_documents = train_df[\"text\"].tolist()\n",
    "train_labels = train_df[\"stars\"].tolist()\n",
    "\n",
    "eval_documents = eval_df[\"text\"].tolist()\n",
    "eval_labels = eval_df[\"stars\"].tolist()\n",
    "\n",
    "test_documents = test_df[\"text\"].tolist()\n",
    "test_labels = test_df[\"stars\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HAN datasets...\n",
      "Loading spacy model...\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 1255411/1255411 [24:13<00:00, 863.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching tokenized documents...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = HANDataset(train_documents, train_labels, batch_size=1000, n_process=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = HANDataset(eval_documents, eval_labels, batch_size=1000, n_process=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HANDataset(test_documents, test_labels, batch_size=1000, n_process=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(train_dataset.vocab)\n",
    "embedding_dim = 200  # Arbitrary Word2Vec embedding size\n",
    "\n",
    "# Hyperparameters for GRU layers\n",
    "word_hidden_dim = 50\n",
    "sent_hidden_dim = 50\n",
    "num_classes = 5  # 1-5 star ratings\n",
    "\n",
    "print(\"Initializing HAN model...\")\n",
    "model = HAN(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    word_hidden_dim,\n",
    "    sent_hidden_dim,\n",
    "    num_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training HAN model...\")\n",
    "train_model(\n",
    "    model, train_dataloader, eval_dataloader, num_epochs=5, lr=1e-3, device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
